import pandas as pdimport osfrom sqlalchemy import engineimport psycopg2import openaiimport signalfrom ast import literal_evalfrom utils.chat_utils import get_response, parse_el, parse_yn, chunk_responsefrom utils.db_utils import PostgresAPIfrom tenacity import (    retry,    stop_after_attempt,    wait_random_exponential,    retry_if_exception_type)  # for exponential backoffdef priv_to_sent(tup):    grantee = tup[0]    priv_type = tup[1]    tbl = tup[2]        sentence = 'The role/user ' + grantee + ' has ' + priv_type + ' access on table/view ' + tbl + '.'    return sentencedef construct_context(db_details):    rel_privs = ['SELECT', 'UPDATE', 'INSERT', 'DELETE', 'CREATE', 'GRANT']    out_sents = []    pgapi = PostgresAPI(db_details)    priv_tups = pgapi.get_privs()        for tup in priv_tups:        if tup[1] in rel_privs:            new_sentence = priv_to_sent(tup)            out_sents.append(new_sentence)        return out_sentsdef llm_answer(question, chatdir, chatpref, outdir, outname, db_details):    if not os.path.exists(outdir):        os.mkdir(outdir)    context = construct_context(db_details)    context_prompt = 'Consider the following privileges implemented in the database:'    prompt = 'Answer the following question about the privileges on the database: ' + question        #TODO: for now, if we need to chunk the prompts, just return the answers from all chunks.    #However, an interesting question is how to aggregate the answers from chunks for questions with open answers.    resps = chunk_response(prompt, context_prompt, context, 'gpt-3.5-turbo', 0.0, chatdir, chatpref)        outfile = os.path.join(outdir, outname + '.txt')    with open(outfile, 'w+') as fh:        print(resps, file=fh)        return resps        